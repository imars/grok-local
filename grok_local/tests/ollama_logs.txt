2025/03/06 05:08:46 routes.go:1205: INFO server config env="map[HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/Users/ian/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://*] OLLAMA_SCHED_SPREAD:false http_proxy: https_proxy: no_proxy:]"
time=2025-03-06T05:08:46.258Z level=INFO source=images.go:432 msg="total blobs: 13"
time=2025-03-06T05:08:46.259Z level=INFO source=images.go:439 msg="total unused blobs removed: 0"
time=2025-03-06T05:08:46.259Z level=INFO source=routes.go:1256 msg="Listening on 127.0.0.1:11434 (version 0.5.12)"
time=2025-03-06T05:08:46.260Z level=INFO source=types.go:130 msg="inference compute" id="" library=cpu variant="" compute="" driver=0.0 name="" total="16.0 GiB" available="7.3 GiB"
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0time=2025-03-06T05:08:51.349Z level=INFO source=server.go:97 msg="system memory" total="16.0 GiB" free="7.3 GiB" free_swap="0 B"
time=2025-03-06T05:08:51.349Z level=WARN source=ggml.go:132 msg="key not found" key=llama.attention.key_length default=128
time=2025-03-06T05:08:51.349Z level=WARN source=ggml.go:132 msg="key not found" key=llama.attention.value_length default=128
time=2025-03-06T05:08:51.349Z level=INFO source=server.go:130 msg=offload library=cpu layers.requested=-1 layers.model=33 layers.offload=0 layers.split="" memory.available="[7.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.0 GiB" memory.required.partial="0 B" memory.required.kv="1.0 GiB" memory.required.allocations="[6.0 GiB]" memory.weights.total="4.9 GiB" memory.weights.repeating="4.5 GiB" memory.weights.nonrepeating="411.0 MiB" memory.graph.full="560.0 MiB" memory.graph.partial="677.5 MiB"
time=2025-03-06T05:08:51.351Z level=INFO source=server.go:380 msg="starting llama server" cmd="/Applications/Ollama.app/Contents/Resources/ollama runner --model /Users/ian/.ollama/models/blobs/sha256-6340dc3229b0d08ea9cc49b75d4098702983e17b4c096d57afbbf2ffc813f2be --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 4 --port 52241"
time=2025-03-06T05:08:51.353Z level=INFO source=sched.go:450 msg="loaded runners" count=1
time=2025-03-06T05:08:51.353Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-03-06T05:08:51.353Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-03-06T05:08:51.375Z level=INFO source=runner.go:932 msg="starting go runner"
load_backend: loaded CPU backend from /Applications/Ollama.app/Contents/Resources/libggml-cpu-haswell.so
time=2025-03-06T05:08:51.400Z level=INFO source=runner.go:935 msg=system info="CPU : SSE3 = 1 | SSSE3 = 1 | LLAMAFILE = 1 | CPU : SSE3 = 1 | SSSE3 = 1 | LLAMAFILE = 1 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | cgo(clang)" threads=2
time=2025-03-06T05:08:51.401Z level=INFO source=runner.go:993 msg="Server listening on 127.0.0.1:52241"
llama_model_loader: loaded meta data with 28 key-value pairs and 292 tensors from /Users/ian/.ollama/models/blobs/sha256-6340dc3229b0d08ea9cc49b75d4098702983e17b4c096d57afbbf2ffc813f2be (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Llama 8B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Llama
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                          llama.block_count u32              = 32
llama_model_loader: - kv   6:                       llama.context_length u32              = 131072
llama_model_loader: - kv   7:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   8:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   9:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  10:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  12:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                          general.file_type u32              = 15
llama_model_loader: - kv  14:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  15:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  21:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 128001
llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 128001
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  27:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
time=2025-03-06T05:08:51.605Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 256
llm_load_vocab: token to piece cache size = 0.7999 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 131072
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 131072
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 8B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW) 
llm_load_print_meta: general.name     = DeepSeek R1 Distill Llama 8B
llm_load_print_meta: BOS token        = 128000 '<｜begin▁of▁sentence｜>'
llm_load_print_meta: EOS token        = 128001 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOT token        = 128001 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'
llm_load_print_meta: PAD token        = 128001 '<｜end▁of▁sentence｜>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 128001 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'
llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'
llm_load_print_meta: max token length = 256
llm_load_tensors:          CPU model buffer size =  4685.30 MiB
100    65    0     0  100    65      0     53  0:00:01  0:00:01 --:--:--    53100    65    0     0  100    65      0     29  0:00:02  0:00:02 --:--:--    29100    65    0     0  100    65      0     20  0:00:03  0:00:03 --:--:--    20100    65    0     0  100    65      0     15  0:00:04  0:00:04 --:--:--    15100    65    0     0  100    65      0     12  0:00:05  0:00:05 --:--:--    12100    65    0     0  100    65      0     10  0:00:06  0:00:06 --:--:--     0100    65    0     0  100    65      0      8  0:00:08  0:00:07  0:00:01     0llama_new_context_with_model: n_seq_max     = 4
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 500000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1
100    65    0     0  100    65      0      7  0:00:09  0:00:08  0:00:01     0llama_kv_cache_init:        CPU KV buffer size =  1024.00 MiB
llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     2.02 MiB
llama_new_context_with_model:        CPU compute buffer size =   560.01 MiB
llama_new_context_with_model: graph nodes  = 1030
llama_new_context_with_model: graph splits = 1
time=2025-03-06T05:08:59.922Z level=INFO source=server.go:596 msg="llama runner started in 8.57 seconds"
100    65    0     0  100    65      0      7  0:00:09  0:00:09 --:--:--     0100    65    0     0  100    65      0      6  0:00:10  0:00:10 --:--:--     0100    65    0     0  100    65      0      5  0:00:13  0:00:11  0:00:02     0100    65    0     0  100    65      0      5  0:00:13  0:00:12  0:00:01     0100    65    0     0  100    65      0      4  0:00:16  0:00:13  0:00:03     0100    65    0     0  100    65      0      4  0:00:16  0:00:14  0:00:02     0100    65    0     0  100    65      0      4  0:00:16  0:00:15  0:00:01     0100    65    0     0  100    65      0      3  0:00:21  0:00:16  0:00:05     0100    65    0     0  100    65      0      3  0:00:21  0:00:17  0:00:04     0100    65    0     0  100    65      0      3  0:00:21  0:00:18  0:00:03     0100    65    0     0  100    65      0      3  0:00:21  0:00:19  0:00:02     0100    65    0     0  100    65      0      3  0:00:21  0:00:20  0:00:01     0[GIN] 2025/03/06 - 05:09:12 | 200 |      86.243µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/03/06 - 05:09:12 | 200 |     344.659µs |       127.0.0.1 | GET      "/api/ps"
100    65    0     0  100    65      0      3  0:00:21  0:00:21 --:--:--     0100    65    0     0  100    65      0      2  0:00:32  0:00:22  0:00:10     0100    65    0     0  100    65      0      2  0:00:32  0:00:23  0:00:09     0100    65    0     0  100    65      0      2  0:00:32  0:00:24  0:00:08     0100    65    0     0  100    65      0      2  0:00:32  0:00:25  0:00:07     0100    65    0     0  100    65      0      2  0:00:32  0:00:26  0:00:06     0100    65    0     0  100    65      0      2  0:00:32  0:00:27  0:00:05     0100    65    0     0  100    65      0      2  0:00:32  0:00:28  0:00:04     0100    65    0     0  100    65      0      2  0:00:32  0:00:29  0:00:03     0100    65    0     0  100    65      0      2  0:00:32  0:00:30  0:00:02     0100    65    0     0  100    65      0      2  0:00:32  0:00:31  0:00:01     0100    65    0     0  100    65      0      2  0:00:32  0:00:32 --:--:--     0100    65    0     0  100    65      0      1  0:01:05  0:00:33  0:00:32     0100    65    0     0  100    65      0      1  0:01:05  0:00:34  0:00:31     0100    65    0     0  100    65      0      1  0:01:05  0:00:35  0:00:30     0100    65    0     0  100    65      0      1  0:01:05  0:00:36  0:00:29     0100    65    0     0  100    65      0      1  0:01:05  0:00:37  0:00:28     0100    65    0     0  100    65      0      1  0:01:05  0:00:38  0:00:27     0100    65    0     0  100    65      0      1  0:01:05  0:00:39  0:00:26     0100    65    0     0  100    65      0      1  0:01:05  0:00:40  0:00:25     0100    65    0     0  100    65      0      1  0:01:05  0:00:41  0:00:24     0100    65    0     0  100    65      0      1  0:01:05  0:00:42  0:00:23     0100    65    0     0  100    65      0      1  0:01:05  0:00:43  0:00:22     0100    65    0     0  100    65      0      1  0:01:05  0:00:44  0:00:21     0100    65    0     0  100    65      0      1  0:01:05  0:00:45  0:00:20     0100    65    0     0  100    65      0      1  0:01:05  0:00:46  0:00:19     0100    65    0     0  100    65      0      1  0:01:05  0:00:47  0:00:18     0100    65    0     0  100    65      0      1  0:01:05  0:00:48  0:00:17     0100    65    0     0  100    65      0      1  0:01:05  0:00:49  0:00:16     0100    65    0     0  100    65      0      1  0:01:05  0:00:50  0:00:15     0100    65    0     0  100    65      0      1  0:01:05  0:00:51  0:00:14     0100    65    0     0  100    65      0      1  0:01:05  0:00:52  0:00:13     0100    65    0     0  100    65      0      1  0:01:05  0:00:53  0:00:12     0100    65    0     0  100    65      0      1  0:01:05  0:00:54  0:00:11     0100    65    0     0  100    65      0      1  0:01:05  0:00:55  0:00:10     0100    65    0     0  100    65      0      1  0:01:05  0:00:56  0:00:09     0100    65    0     0  100    65      0      1  0:01:05  0:00:57  0:00:08     0100    65    0     0  100    65      0      1  0:01:05  0:00:58  0:00:07     0100    65    0     0  100    65      0      1  0:01:05  0:00:59  0:00:06     0100    65    0     0  100    65      0      1  0:01:05  0:01:00  0:00:05     0100    65    0     0  100    65      0      1  0:01:05  0:01:01  0:00:04     0100    65    0     0  100    65      0      1  0:01:05  0:01:02  0:00:03     0100    65    0     0  100    65      0      1  0:01:05  0:01:03  0:00:02     0100    65    0     0  100    65      0      1  0:01:05  0:01:04  0:00:01     0100    65    0     0    0    65      0      0 --:--:--  0:01:05 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:06 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:07 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:08 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:09 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:10 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:11 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:12 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:13 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:14 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:15 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:16 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:17 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:18 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:19 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:20 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:21 --:--:--     0[GIN] 2025/03/06 - 05:10:13 | 200 |      61.541µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/03/06 - 05:10:13 | 200 |      53.985µs |       127.0.0.1 | GET      "/api/ps"
100    65    0     0    0    65      0      0 --:--:--  0:01:22 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:23 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:24 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:25 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:26 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:27 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:28 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:29 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:30 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:31 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:32 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:33 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:34 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:35 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:36 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:37 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:38 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:39 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:40 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:41 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:42 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:43 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:44 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:45 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:46 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:47 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:48 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:49 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:50 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:51 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:52 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:53 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:54 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:55 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:56 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:57 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:58 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:01:59 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:00 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:01 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:02 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:03 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:04 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:05 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:06 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:07 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:08 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:09 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:10 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:11 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:12 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:13 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:14 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:15 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:16 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:17 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:18 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:19 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:20 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:21 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:22 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:23 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:24 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:25 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:26 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:27 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:28 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:29 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:30 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:31 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:32 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:34 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:35 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:36 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:37 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:38 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:39 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:40 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:41 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:42 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:43 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:44 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:45 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:46 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:47 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:48 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:49 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:50 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:51 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:52 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:53 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:54 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:55 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:56 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:57 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:58 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:02:59 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:00 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:01 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:02 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:03 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:04 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:05 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:06 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:07 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:08 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:09 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:10 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:11 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:12 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:13 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:14 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:15 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:16 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:17 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:18 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:19 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:20 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:21 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:22 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:23 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:24 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:25 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:26 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:27 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:28 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:29 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:30 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:31 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:32 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:33 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:34 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:35 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:36 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:37 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:38 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:39 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:40 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:41 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:42 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:43 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:44 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:45 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:46 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:47 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:48 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:49 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:50 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:51 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:52 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:53 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:54 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:55 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:56 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:57 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:58 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:03:59 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:04:00 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:04:01 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:04:02 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:04:03 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:04:04 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:04:05 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:04:06 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:04:07 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:04:08 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:04:09 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:04:10 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:04:11 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:04:12 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:04:13 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:04:14 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:04:15 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:04:16 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:04:17 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:04:18 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:04:19 --:--:--     0100    65    0     0    0    65      0      0 --:--:--  0:04:20 --:--:--     0llama_model_loader: loaded meta data with 28 key-value pairs and 292 tensors from /Users/ian/.ollama/models/blobs/sha256-6340dc3229b0d08ea9cc49b75d4098702983e17b4c096d57afbbf2ffc813f2be (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Llama 8B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Llama
llama_model_loader: - kv   4:                         general.size_label str              = 8B
llama_model_loader: - kv   5:                          llama.block_count u32              = 32
llama_model_loader: - kv   6:                       llama.context_length u32              = 131072
llama_model_loader: - kv   7:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   8:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   9:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  10:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  12:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                          general.file_type u32              = 15
llama_model_loader: - kv  14:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  15:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  21:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 128001
llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 128001
llama_model_loader: - kv  24:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  25:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  27:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   66 tensors
llama_model_loader: - type q4_K:  193 tensors
llama_model_loader: - type q6_K:   33 tensors
100    65    0     0    0    65      0      0 --:--:--  0:04:21 --:--:--     0llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 256
llm_load_vocab: token to piece cache size = 0.7999 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: vocab_only       = 1
llm_load_print_meta: model type       = ?B
llm_load_print_meta: model ftype      = all F32
llm_load_print_meta: model params     = 8.03 B
llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW) 
llm_load_print_meta: general.name     = DeepSeek R1 Distill Llama 8B
llm_load_print_meta: BOS token        = 128000 '<｜begin▁of▁sentence｜>'
llm_load_print_meta: EOS token        = 128001 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOT token        = 128001 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'
llm_load_print_meta: PAD token        = 128001 '<｜end▁of▁sentence｜>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 128001 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'
llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'
llm_load_print_meta: max token length = 256
llama_model_load: vocab only - skipping tensors
[GIN] 2025/03/06 - 05:13:12 | 200 |         4m21s |       127.0.0.1 | POST     "/api/generate"
{"model":"deepseek-r1:8b","created_at":"2025-03-06T05:13:11.978764Z","response":"\u003cthink\u003e\nOkay, so I just saw this message about a warm-up. It says \"It's getting cold outside, but you can still keep things hot in the kitchen with these easy-to-follow recipes and tips.\" Then it asks if I want to get started or check out the full list.\n\nAlright, first thing, I need to figure out what exactly they're offering. They mentioned recipes and tips that help keep things hot in the kitchen during cold weather. That probably means comfort food or hearty dishes that are perfect for colder months.\n\nI wonder what kind of recipes they have. Maybe soups, stews, roasts, or something like that? I should think about what people usually cook when it's chilly outside. Soup is a classic, as is chili, maybe some baked goods too.\n\nAlso, the message seems to be encouraging interaction. They ask if I want to get started or check out the full list. So maybe they have multiple recipes or tips categorized in some way. Perhaps by meal type, like breakfast, lunch, dinner, or appetizers.\n\nI should consider how to respond. If I say \"Yes,\" what does that mean? Do they provide step-by-step instructions for each recipe? Or just the ingredients and ideas?\n\nAnother thought: maybe the warm-up is part of a larger series or newsletter. So they're probably building a list of people interested in these types of recipes. I should be cautious about sharing personal info, but since this seems friendly, it's likely safe.\n\nI also think about the time of year when this message is sent. If it's winter, they might focus on things that take longer to prepare, like roasts or slow-cooked meals. Or maybe include some comfort food like mac and cheese or casseroles.\n\nWait, but if it's a warm-up, maybe it's more about the temperature in the kitchen. So recipes that generate heat, like baking cookies or heating up meals. But that might be too literal.\n\nAlternatively, maybe they're referring to keeping the kitchen warm by cooking, which makes sense because cooking involves turning on the stove or oven, which can help keep the house warm.\n\nI should also consider the audience. Is this for home cooks, professionals, families? The message doesn't specify, so it's probably for a general audience, maybe families looking for meal ideas during colder months.\n\nIn terms of tips, they might include things like using leftovers to make meals without cooking from scratch, or how to store food properly to keep it fresh longer. Or maybe energy-saving tips when cooking during winter, like using slow cookers instead of ovens.\n\nI'm also thinking about the structure of the response. If I reply \"Yes,\" what comes next? Do they provide a specific recipe first, or a list of all the recipes and tips? It would be helpful to know so I can prepare accordingly.\n\nAnother angle: maybe this warm-up is part of a promotion or challenge, like a cooking challenge for the season. They might be encouraging people to share their recipes too, creating a community aspect.\n\nI should also think about potential follow-up actions. If they send me recipes, how do I provide feedback? Is there a way to rate them or ask for more details? The message doesn't specify, so maybe just replying with \"Yes\" is enough for now.\n\nLastly, considering all this, the response seems straightforward: reply positively to receive the information. I don't need to overcomplicate it unless I have specific questions about the recipes or tips.\n\u003c/think\u003e\n\n**Response:**\n\nThe warm-up message offers a selection of recipes and tips designed to keep your kitchen cozy during colder months. These likely include hearty dishes like soups, stews, roasts, and comfort foods perfect for the season. If you're interested in exploring these options, replying with \"Yes\" will provide access to the detailed recipes and tips.\n\n**Reply:**\n\n\"Yes, I'd love to get started! Please share the easy-to-follow recipes and tips for keeping things hot in the kitchen this winter.\"","done":true,"done_reason":"stop","con100  8122    0  8057    0    65     30      0 --:--:--  0:04:21 --:--:--  1961
text":[128011,96375,5352,128012,128013,198,33413,11,779,358,1120,5602,420,1984,922,264,8369,5352,13,1102,2795,330,2181,596,3794,9439,4994,11,719,499,649,2103,2567,2574,4106,304,279,9979,449,1521,4228,4791,93685,19141,323,10631,1210,5112,433,17501,422,358,1390,311,636,3940,477,1817,704,279,2539,1160,382,72586,11,1176,3245,11,358,1205,311,7216,704,1148,7041,814,2351,10209,13,2435,9932,19141,323,10631,430,1520,2567,2574,4106,304,279,9979,2391,9439,9282,13,3011,4762,3445,6981,3691,477,82651,26863,430,527,4832,369,76214,4038,382,40,5895,1148,3169,315,19141,814,617,13,10926,5945,1725,11,357,28844,11,938,12019,11,477,2555,1093,430,30,358,1288,1781,922,1148,1274,6118,4394,994,433,596,81975,4994,13,61434,374,264,11670,11,439,374,59717,11,7344,1063,41778,11822,2288,382,13699,11,279,1984,5084,311,387,26921,16628,13,2435,2610,422,358,1390,311,636,3940,477,1817,704,279,2539,1160,13,2100,7344,814,617,5361,19141,477,10631,71974,304,1063,1648,13,19292,555,15496,955,11,1093,17954,11,16163,11,14177,11,477,30168,12509,382,40,1288,2980,1268,311,6013,13,1442,358,2019,330,9642,1359,1148,1587,430,3152,30,3234,814,3493,3094,14656,30308,11470,369,1855,11363,30,2582,1120,279,14293,323,6848,1980,14364,3463,25,7344,279,8369,5352,374,961,315,264,8294,4101,477,20846,13,2100,814,2351,4762,4857,264,1160,315,1274,8173,304,1521,4595,315,19141,13,358,1288,387,46878,922,11821,4443,3630,11,719,2533,420,5084,11919,11,433,596,4461,6220,382,40,1101,1781,922,279,892,315,1060,994,420,1984,374,3288,13,1442,433,596,12688,11,814,2643,5357,389,2574,430,1935,5129,311,10772,11,1093,938,12019,477,6435,23283,11059,21644,13,2582,7344,2997,1063,6981,3691,1093,9155,323,17604,477,45324,2382,645,382,14524,11,719,422,433,596,264,8369,5352,11,7344,433,596,810,922,279,9499,304,279,9979,13,2100,19141,430,7068,8798,11,1093,28915,8443,477,24494,709,21644,13,2030,430,2643,387,2288,24016,382,93114,11,7344,814,2351,22797,311,10494,279,9979,8369,555,17677,11,902,3727,5647,1606,17677,18065,13353,389,279,45115,477,24276,11,902,649,1520,2567,279,3838,8369,382,40,1288,1101,2980,279,10877,13,2209,420,369,2162,73527,11,15749,11,8689,30,578,1984,3250,956,14158,11,779,433,596,4762,369,264,4689,10877,11,7344,8689,3411,369,15496,6848,2391,76214,4038,382,644,3878,315,10631,11,814,2643,2997,2574,1093,1701,93267,311,1304,21644,2085,17677,505,19307,11,477,1268,311,3637,3691,10489,311,2567,433,7878,5129,13,2582,7344,4907,59085,10631,994,17677,2391,12688,11,1093,1701,6435,4394,388,4619,315,297,21778,382,40,2846,1101,7422,922,279,6070,315,279,2077,13,1442,358,10052,330,9642,1359,1148,4131,1828,30,3234,814,3493,264,3230,11363,1176,11,477,264,1160,315,682,279,19141,323,10631,30,1102,1053,387,11190,311,1440,779,358,649,10772,28178,382,14364,9392,25,7344,420,8369,5352,374,961,315,264,20862,477,8815,11,1093,264,17677,8815,369,279,3280,13,2435,2643,387,26921,1274,311,4430,872,19141,2288,11,6968,264,4029,13189,382,40,1288,1101,1781,922,4754,1833,5352,6299,13,1442,814,3708,757,19141,11,1268,656,358,3493,11302,30,2209,1070,264,1648,311,4478,1124,477,2610,369,810,3649,30,578,1984,3250,956,14158,11,779,7344,1120,2109,6852,449,330,9642,1,374,3403,369,1457,382,81586,11,13126,682,420,11,279,2077,5084,31439,25,10052,40646,311,5371,279,2038,13,358,1541,956,1205,311,927,5807,49895,433,7389,358,617,3230,4860,922,279,19141,477,10631,627,128014,271,334,2647,25,57277,791,8369,5352,1984,6209,264,6727,315,19141,323,10631,6319,311,2567,701,9979,43535,2391,76214,4038,13,4314,4461,2997,82651,26863,1093,5945,1725,11,357,28844,11,938,12019,11,323,6981,15657,4832,369,279,3280,13,1442,499,2351,8173,304,24919,1521,2671,11,2109,6852,449,330,9642,1,690,3493,2680,311,279,11944,19141,323,10631,382,334,21509,25,57277,58841,11,358,4265,3021,311,636,3940,0,5321,4430,279,4228,4791,93685,19141,323,10631,369,10494,2574,4106,304,279,9979,420,12688,1210],"total_duration":260692638108,"load_duration":8635797738,"prompt_eval_count":5,"prompt_eval_duration":3385000000,"eval_count":818,"eval_duration":248669000000}[GIN] 2025/03/06 - 05:14:37 | 200 |      21.093µs |       127.0.0.1 | HEAD     "/"
time=2025-03-06T05:14:37.897Z level=WARN source=routes.go:901 msg="bad manifest filepath" name=registry.ollama.ai/library/crewai-llama3:8b error="open /Users/ian/.ollama/models/blobs/sha256-db1eafba595c0fb81c9c9b26b975dda38ee4834c9ab2731d9817d54d33b743bb: no such file or directory"
time=2025-03-06T05:14:37.897Z level=WARN source=routes.go:901 msg="bad manifest filepath" name=registry.ollama.ai/library/llama3:8b error="open /Users/ian/.ollama/models/blobs/sha256-3f8eb4da87fa7a3c9da615036b0dc418d31fef2a30b115ff33562588b32c691d: no such file or directory"
time=2025-03-06T05:14:37.898Z level=WARN source=routes.go:901 msg="bad manifest filepath" name=registry.ollama.ai/library/llama3:latest error="open /Users/ian/.ollama/models/blobs/sha256-3f8eb4da87fa7a3c9da615036b0dc418d31fef2a30b115ff33562588b32c691d: no such file or directory"
time=2025-03-06T05:14:37.898Z level=WARN source=routes.go:901 msg="bad manifest filepath" name=registry.ollama.ai/library/llama3.1:latest error="open /Users/ian/.ollama/models/blobs/sha256-e711233e734332fe5f8a09b2407fb5a083e39ca7e0ba90788026414cd4c059af: no such file or directory"
[GIN] 2025/03/06 - 05:14:37 | 200 |    6.411149ms |       127.0.0.1 | GET      "/api/tags"
NAME                  ID              SIZE      MODIFIED    
deepseek-r1:8b        b06f7ffc236b    4.9 GB    2 weeks ago    
deepseek-r1:latest    0a8c26691023    4.7 GB    3 weeks ago    
llama3.2:latest       a80c4f17acd5    2.0 GB    6 weeks ago    
time=2025-03-06T05:14:40.511Z level=INFO source=server.go:97 msg="system memory" total="16.0 GiB" free="6.1 GiB" free_swap="0 B"
time=2025-03-06T05:14:40.511Z level=INFO source=server.go:130 msg=offload library=cpu layers.requested=-1 layers.model=29 layers.offload=0 layers.split="" memory.available="[6.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="3.3 GiB" memory.required.partial="0 B" memory.required.kv="896.0 MiB" memory.required.allocations="[3.3 GiB]" memory.weights.total="2.4 GiB" memory.weights.repeating="2.1 GiB" memory.weights.nonrepeating="308.2 MiB" memory.graph.full="424.0 MiB" memory.graph.partial="570.7 MiB"
time=2025-03-06T05:14:40.513Z level=INFO source=server.go:380 msg="starting llama server" cmd="/Applications/Ollama.app/Contents/Resources/ollama runner --model /Users/ian/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 4 --port 52258"
time=2025-03-06T05:14:40.519Z level=INFO source=sched.go:450 msg="loaded runners" count=2
time=2025-03-06T05:14:40.520Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-03-06T05:14:40.520Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-03-06T05:14:40.545Z level=INFO source=runner.go:932 msg="starting go runner"
load_backend: loaded CPU backend from /Applications/Ollama.app/Contents/Resources/libggml-cpu-haswell.so
time=2025-03-06T05:14:40.571Z level=INFO source=runner.go:935 msg=system info="CPU : SSE3 = 1 | SSSE3 = 1 | LLAMAFILE = 1 | CPU : SSE3 = 1 | SSSE3 = 1 | LLAMAFILE = 1 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | cgo(clang)" threads=2
time=2025-03-06T05:14:40.571Z level=INFO source=runner.go:993 msg="Server listening on 127.0.0.1:52258"
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /Users/ian/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
time=2025-03-06T05:14:40.772Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llm_load_vocab: special tokens cache size = 256
llm_load_vocab: token to piece cache size = 0.7999 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 131072
llm_load_print_meta: n_embd           = 3072
llm_load_print_meta: n_layer          = 28
llm_load_print_meta: n_head           = 24
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 3
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 131072
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 3B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 3.21 B
llm_load_print_meta: model size       = 1.87 GiB (5.01 BPW) 
llm_load_print_meta: general.name     = Llama 3.2 3B Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'
llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'
llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'
llm_load_print_meta: max token length = 256
llm_load_tensors:          CPU model buffer size =  1918.35 MiB
llama_new_context_with_model: n_seq_max     = 4
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 500000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1
llama_kv_cache_init:        CPU KV buffer size =   896.00 MiB
llama_new_context_with_model: KV self size  =  896.00 MiB, K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     2.00 MiB
llama_new_context_with_model:        CPU compute buffer size =   424.01 MiB
llama_new_context_with_model: graph nodes  = 902
llama_new_context_with_model: graph splits = 1
time=2025-03-06T05:14:46.888Z level=INFO source=server.go:596 msg="llama runner started in 6.37 seconds"
llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /Users/ian/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2
llama_model_loader: - kv   5:                         general.size_label str              = 3B
llama_model_loader: - kv   6:                               general.tags arr[str,6]       = ["facebook", "meta", "pytorch", "llam...
llama_model_loader: - kv   7:                          general.languages arr[str,8]       = ["en", "de", "fr", "it", "pt", "hi", ...
llama_model_loader: - kv   8:                          llama.block_count u32              = 28
llama_model_loader: - kv   9:                       llama.context_length u32              = 131072
llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072
llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24
llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128
llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128
llama_model_loader: - kv  18:                          general.file_type u32              = 15
llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  29:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   58 tensors
llama_model_loader: - type q4_K:  168 tensors
llama_model_loader: - type q6_K:   29 tensors
llm_load_vocab: special tokens cache size = 256
llm_load_vocab: token to piece cache size = 0.7999 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: vocab_only       = 1
llm_load_print_meta: model type       = ?B
llm_load_print_meta: model ftype      = all F32
llm_load_print_meta: model params     = 3.21 B
llm_load_print_meta: model size       = 1.87 GiB (5.01 BPW) 
llm_load_print_meta: general.name     = Llama 3.2 3B Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'
llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'
llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'
llm_load_print_meta: max token length = 256
llama_model_load: vocab only - skipping tensors
[GIN] 2025/03/06 - 05:15:04 | 200 | 23.987913352s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/03/06 - 05:17:04 | 500 |          2m0s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/03/06 - 05:17:25 | 200 |  7.987837655s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/03/06 - 05:19:25 | 500 |          2m0s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/03/06 - 05:19:47 | 200 |  9.581152151s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/03/06 - 05:21:47 | 500 |          2m0s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/03/06 - 05:21:49 | 200 |       38.49µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/03/06 - 05:21:49 | 200 |      61.235µs |       127.0.0.1 | GET      "/api/ps"
